{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9GCGMtwyohS"
      },
      "source": [
        "# Week 8: Convolutional Neural Networks (CNN)\n",
        "\n",
        "In this lab, we will cover convolutions and pooling in order to create CNNs. We also build a model to classify digit images using the MNIST dataset. Last, you will be asked to create a CNN model for a flower classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRp_qWdxyohW"
      },
      "source": [
        "## Basic Imports\n",
        "\n",
        "For this lab session, we will be needing Keras, Numpy and Tensorflow. <br>\n",
        "We will build our CNN in Keras, but first we need to understand the underlying principles, for which we will use Tensorflow. <br>\n",
        "Numpy will be mainly used for dataset preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5P4TN6uLyohX"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "from numpy.random import seed\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coF3bUaByohe"
      },
      "source": [
        "## Replicability\n",
        "While experimenting and researching, it is important that your results can be __replicated by other people__. <br>\n",
        "To ensure some level of replicability, we can __set the starting seed__ of both numpy and tensorflow __to known value__. <br>\n",
        "Therefore, when we initialise our network to random values, these states can be calculated and replicated just by knowing the seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "68_hPQu8yohf"
      },
      "outputs": [],
      "source": [
        "seed(101)\n",
        "tf.random.set_seed(101)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE6znDdhyohj"
      },
      "source": [
        "## Low Level Code\n",
        "\n",
        "While building a CNN model, we will add whole convolution layer in a neat package `Conv2D()`. <br>\n",
        "But before we do so, it is good to understand the __underlaying mechanics__ and __code implementation__ of convolutions. <br>\n",
        "In the following example, we will define our image/data array `inputs`, and using `kernel` to apply 2D convolution. <br>\n",
        "\n",
        "\n",
        "A convolution input must have shape of `(BatchSize, width, height, inputChannels)` <br>\n",
        "A convolution filter must have shape of `(width, height, inputChannels, outputChannels)` <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMy6HP4Wyohi"
      },
      "source": [
        "# Convolutions\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are designed to learn features directly from image pixels. They can classify patterns or objects with extreme variability. Currently, they form the core of various __computer vision systems__ such as Facebook's automated photo tagging, handwritten characters recognition, self-driving cars, marine mammal detection, and medical image analysis. In this lab, we will start by exploring a convolution function which forms the heart of CNNs.\n",
        "\n",
        "![image.png](https://global.discourse-cdn.com/business5/uploads/pynq1/original/2X/9/984a55715f7e36bdf068ee82d23a204d398f8561.jpeg)\n",
        "\n",
        "### Task\n",
        "1) Experiment with different strides, kernel, padding etc. <br>\n",
        "2) Why does first ouput element equal to 21.0?\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first element of the output is 21 as this is the result of applying the kernel to the top left (1st position) of the input.\n",
        "This gives us the values 1 + 7 + 13 = 21"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EDUa_aP1yohj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of an Input: (1, 5, 5, 1)\n",
            "Shape of a Kernel (3, 3, 1, 1)\n",
            "Shape of result: (1, 3, 2, 1)\n",
            "Result: [[[[21.]\n",
            "   [27.]]\n",
            "\n",
            "  [[36.]\n",
            "   [42.]]\n",
            "\n",
            "  [[51.]\n",
            "   [57.]]]]\n"
          ]
        }
      ],
      "source": [
        "# We need keras.backend and tensorflow to create proper tensors directly\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "inputs = tf.constant([[1.0,  2.0, 3.0,  4.0, 5.0],\n",
        "            [6.0,  7.0, 8.0,  9.0, 10.0],\n",
        "            [11.0, 12.0, 13.0, 14.0, 15.0],\n",
        "            [16.0, 17.0, 18.0, 19.0, 20.0],\n",
        "            [21.0, 22.0, 23.0, 24.0, 25.0],\n",
        "            ])\n",
        "\n",
        "\n",
        "kernel = tf.constant([[1.0,0.0,0.0],\n",
        "            [0.0,1.0,0.0],\n",
        "            [0.0,0.0,1.0]\n",
        "            ])\n",
        "\n",
        "\n",
        "# -1 here in reshape is a placeholder, when we are not sure which value we want to use for the 1st dimension, keras infers it for us\n",
        "inputs = K.reshape(inputs,(-1,5,5,1))\n",
        "print(\"Shape of an Input:\", inputs.shape)\n",
        "\n",
        "kernel =K.reshape(kernel,(3,3,1,1))\n",
        "print(\"Shape of a Kernel\", kernel.shape)\n",
        "\n",
        "strides=(1, 2)\n",
        "padding='valid'\n",
        "\n",
        "result = K.conv2d(inputs, kernel, strides=strides, padding=padding)\n",
        "print(\"Shape of result:\", result.shape)\n",
        "\n",
        "print(\"Result:\", K.eval(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12ATZEXIyoho"
      },
      "source": [
        "# Pooling\n",
        "\n",
        "## MaxPooling\n",
        "\n",
        "Another important concept of CNNs is max-pooling, which is a form of\n",
        "non-linear down-sampling. Max-pooling partitions the input image into a\n",
        "set of non-overlapping rectangles and, for each such sub-region, outputs\n",
        "the maximum value.\n",
        "Max-pooling is useful in vision for two reasons:\n",
        "- By eliminating non-maximal values, it reduces computation for upper layers.\n",
        "- It provides a form of translation invariance.\n",
        "\n",
        "\n",
        "## AveragePooling\n",
        "Alternative to MaxPooling is Average pooling, where you take sum of all elements in pool and divide by number of elements. \n",
        "\n",
        "Experiment with different strides, kernel, padding etc. <br> \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "520X_-ziyoho"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: (1, 5, 5, 1)\n",
            "Output shape: (1, 4, 4, 1)\n",
            "Result: [[[[ 7.]\n",
            "   [ 8.]\n",
            "   [ 9.]\n",
            "   [10.]]\n",
            "\n",
            "  [[12.]\n",
            "   [13.]\n",
            "   [14.]\n",
            "   [15.]]\n",
            "\n",
            "  [[17.]\n",
            "   [18.]\n",
            "   [19.]\n",
            "   [20.]]\n",
            "\n",
            "  [[22.]\n",
            "   [23.]\n",
            "   [24.]\n",
            "   [25.]]]]\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.constant([[1.0,  2.0, 3.0,  4.0, 5.0],\n",
        "            [6.0,  7.0, 8.0,  9.0, 10.0],\n",
        "            [11.0, 12.0, 13.0, 14.0, 15.0],\n",
        "            [16.0, 17.0, 18.0, 19.0, 20.0],\n",
        "            [21.0, 22.0, 23.0, 24.0, 25.0],\n",
        "            ])\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,5,5,1))\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "\n",
        "pool_size = (2, 2)\n",
        "strides=(1, 1)\n",
        "padding='valid'\n",
        "pool_mode='max' # or use 'avg'\n",
        "\n",
        "result = K.pool2d(inputs, pool_size=pool_size, strides = strides,\n",
        "                          padding = padding,\n",
        "                          pool_mode = pool_mode)\n",
        "\n",
        "print(\"Output shape:\", result.shape)\n",
        "print(\"Result:\", K.eval(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PZ0c4OppRLv"
      },
      "source": [
        "# Stride and Padding\n",
        "- You might have noticed the padding and stride, but do you know what the stride and padding exactly are?\n",
        "- Check this page for some visualisations https://ezyang.github.io/convolution-visualizer/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mohrx8gns73e"
      },
      "source": [
        "## <font color='red'>Task 1</font>\n",
        "We have a input of (1x4x4x1)\n",
        "- Can you make a Conv2D that give the output that is the same size as the input?\n",
        "- Can you make the output a half smaller than the original output? `(1x2x2x1)`\n",
        "- You can verify by printing out the result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Cf7qUPM0s3zk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of an Input: (1, 4, 4, 1)\n",
            "The result is  (1, 4, 4, 1)\n",
            "(1, 3, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0]],\n",
        "                      [[6.0],  [7.0],  [8.0],  [9.0]],\n",
        "                      [[16.0], [17.0], [18.0], [19.0]],\n",
        "                      [[21.0], [22.0], [23.0], [24.0]]\n",
        "                     ])\n",
        "\n",
        "inputs = K.reshape(inputs,(-1,4,4,1))\n",
        "print(\"Shape of an Input:\", inputs.shape)\n",
        "# Your code here (make the same size)\n",
        "# Make sure your output's shape is the same as the input 1x4x4x1\n",
        "kernel = tf.constant([[1.0,0.0,0.0],\n",
        "            [0.0,1.0,0.0],\n",
        "            [0.0,0.0,1.0]\n",
        "            ])\n",
        "kernel =K.reshape(kernel,(3,3,1,1))\n",
        "\n",
        "strides=(1,1)\n",
        "\n",
        "#using padding ='same' ensures that the output will have the same spatial resolution (size) as the input\n",
        "result = K.conv2d(inputs, kernel, strides=strides, padding='same')\n",
        "print(\"The result is \",result.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Your code here (make a half smaller)\n",
        "# Make sure your output's shape is half of the input 1x2x2x1\n",
        "kernel = tf.constant([[1.0,0.0,1.0],\n",
        "            [0.0,1.0,0.0],\n",
        "            [0.0,0.0,1.0]\n",
        "            ])\n",
        "kernel =K.reshape(kernel,(2,2,1,1))\n",
        "result = K.conv2d(result, kernel, strides = strides, padding = 'valid')\n",
        "print(result.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4MxgHrMxvhd"
      },
      "source": [
        "# Using these techniques in a Keras Sequential model\n",
        "\n",
        "Keras provides convolution and [pooling](https://keras.io/api/layers/pooling_layers/) layers, which can be used as-is, such as [Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/).\n",
        "\n",
        "Pay close attention to the input and output shapes, as well as the optional parameters which may come in handy.\n",
        "\n",
        "We will walk through building a CNN classifier using the MNIST dataset.\n",
        "\n",
        "\n",
        "## <font color='red'>Tasks</font>\n",
        "- Please load the dataset using Keras (hint [check the API](https://keras.io/api/datasets/mnist/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk-AN_dwyoht"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "# Your code here, replace None with mnist data\n",
        "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = mnist.load_data()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VYVCzZ7yohv"
      },
      "source": [
        "## Visualizing the dataset\n",
        "We will use __matplot library__ to display an image from the MNIST dataset. <br>\n",
        "\n",
        "Since we have __grayscale__ image, we need to specify that, while displaying it via `cmap='gray'`\n",
        "## <font color='red'>Task</font>\n",
        "- Please try to visualize more image data, so you can grasp what is in it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zStjuHIyohw"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(X_train_orig[0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFMVC3S-oyI7"
      },
      "outputs": [],
      "source": [
        "# Your code here, try to plot another image, maybe X_train_orig[1] or other image you like\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djhb9544ra4b"
      },
      "outputs": [],
      "source": [
        "# Try to print the raw value from data, to see what is it like\n",
        "\n",
        "\n",
        "\n",
        "# Try to print the size a image, so you might know how it looks numerically\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rosLmrd4yohx"
      },
      "source": [
        "# Shaping dataset\n",
        "\n",
        "## Images\n",
        "Currently, the X part of dataset is in form `(number_of_samples, px_width, px_height)` <br>\n",
        "There is one implied information about the dataset, but we need to directly specify it. This information is regarding number of channels per image. Since the MNIST dataset is only greyscale, we need to specify it in the dimensionality of the dataset.\n",
        "Therefore, we need to convert it from `(60000, 28, 28)` to `(60000,28,28,1)`, where `1` stands for greyscale. <br>\n",
        "If we had an RGB image, the shape of the dataset would look like this `(60000,28,28,3)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPL8UqHuyohy"
      },
      "outputs": [],
      "source": [
        "X_train = X_train_orig.reshape(60000,28,28,1)\n",
        "X_test = X_test_orig.reshape(10000,28,28,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xkIR7OByohz"
      },
      "source": [
        "## Labels\n",
        "\n",
        "Label for each image is in form of an __integer__ ranging from 0 to 9. <br>\n",
        "We can use a __one hot encoding__ to transform them into a __binary matrix__. We know there are 10\n",
        "classes for this problem, so we can expect the binary matrix to have a width\n",
        "of 10.\n",
        "\n",
        "### Converting labels to one-hot representation\n",
        "y_train_orig[0] <b>before</b> conversion is <b>[5]</b> <br>\n",
        "y_train_orig[0] <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfSr-xokyohz"
      },
      "outputs": [],
      "source": [
        "print(y_train_orig[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YpOp9q-yoh1"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "to_categorical(y_train_orig[0], 10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgPHFy_eyoh2"
      },
      "source": [
        "### Converting all labels to one-hot matrix\n",
        "We will use the same `to_categorical` function to convert the whole dataset into matrix of one-hot encodings.\n",
        "\n",
        "1) What is the shape of newly created dataset? <br>\n",
        "2) [Optional] Instead of pre-made `to_categorical` function, can you code your own with same functionality?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QmtRZvLyoh2"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(y_train_orig)\n",
        "y_test = to_categorical(y_test_orig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijrxqSVzyoh3"
      },
      "source": [
        "# Building the model\n",
        "In this section we will combine previously demonstrated mechanisms into one system. <br>\n",
        "\n",
        "For this very simple model, we will be using `Conv2D`, `Flatten`, `MaxPool2D' and `Dense` layers. `Dense` = fully-connected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIuFd9mTyoh4"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
        "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation=\"softmax\"))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf0bR7ZCyoh5"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=3, batch_size=64)  # Notice the handy validation_split parameter, no need to create validation splits manually!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qmw5Hhyyoh6"
      },
      "source": [
        "## Data Normalization\n",
        "Though we can observe that the neural network is learning, the rate is __very slow__ and __learning rate deteriorates very quickly__. <br>\n",
        "This behaviour is due to extreme differences between `max` (255) and `min` (0) values of our dataset. <br>\n",
        "Neural networks are performing __best when dataset ranges from 0 to 1__, or in some cases -1 to 1. <br>\n",
        "Since we can imagine these values as signal strength, very high values, such as 255, are way too overpowering and strengthening non-optimal paths too quickly. <br>\n",
        "Therefore we need to divide our training and testing dataset by 255 to get values ranging from 0 to 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtcCvFS_yoh6"
      },
      "outputs": [],
      "source": [
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcv3HyKjyoh7"
      },
      "source": [
        "## Training on Normalized dataset\n",
        "We will generate a new model and train it on normalized dataset. <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbhdHrB_yoh7"
      },
      "outputs": [],
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=4, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D8rLN0f2dLu"
      },
      "source": [
        "And to evaluate, we can use `model.evaluate` which:\n",
        "\n",
        "> Returns the loss value & metrics values for the model in test mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4RDLBYs2fZj"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXn5Tj5LQ_Ci"
      },
      "source": [
        "Now, try to create a new model by yourself with different layers (including a pooling layer) and train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8E-pZMXRKuk"
      },
      "outputs": [],
      "source": [
        "def create_my_model():\n",
        "    model = Sequential()\n",
        "    # your model here\n",
        "    # for a pooling layer\n",
        "    # model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "my_model = create_my_model()\n",
        "#your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.evaluate(X_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGIIpfC3yoiC"
      },
      "source": [
        "### Tensorflow Playground\n",
        "\n",
        "To better visualise the importance of feature maps and feature extraction you can visit the following website and experiment with the structure of the network, hyperparameters to get instant visual feedback and see how your changes reflect the detection of features:\n",
        "\n",
        "[Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.77793&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8n6keI02yIv"
      },
      "source": [
        "# Challenge Part: Flowers\n",
        "The \"102 Category Flower Dataset\" is a flower data set captured by Oxford https://www.robots.ox.ac.uk/~vgg/data/flowers/102/. The authors of the dataset help to visualise the classes via:\n",
        "\n",
        "Shape Isomap\n",
        "\n",
        "![image](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/T_shapeiso.jpg)\n",
        "\n",
        "Colour Isomap\n",
        "\n",
        "![image](https://www.robots.ox.ac.uk/~vgg/data/flowers/102/T_colouriso.jpg)\n",
        "\n",
        "From this, we can see that colour data could be very helpful to our model.\n",
        "\n",
        "In this lab, we will only use the images of 2 flowers (artichoke and buttercup) for training and testing a CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWOg2HxiE68v"
      },
      "source": [
        "Now download the dataset from SurreyLearn (https://surreylearn.surrey.ac.uk/d2l/le/lessons/239765/lessons/2699259) and upload the Lab8.zip file to the Google Colab file folder (the file folder icon in the left sidebar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmMXtL1J6doS"
      },
      "outputs": [],
      "source": [
        "!unzip Lab8.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtMomrUO6d08"
      },
      "source": [
        "We can use the [`image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) utility function to load the images. This is handy because it won't load all the images at once (which can cause you to run out of memory: if using Colab, see the RAM indicator in the toolbar).\n",
        "\n",
        "Now to load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbyZNcDO8E5C"
      },
      "outputs": [],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "\n",
        "# Up to you: RGB is default, but perhaps colours aren't that helpful...?\n",
        "colour_mode = 'rgb'\n",
        "\n",
        "# Up to you: change the image size if desired. Perhaps they don't need to be that big.\n",
        "image_size = (100, 100)\n",
        "\n",
        "# Up to you: training batch size.\n",
        "batch_size = 12\n",
        "\n",
        "train_flowers_dataset = image_dataset_from_directory(\n",
        "  'ABFlowerTrain',\n",
        "  color_mode=colour_mode,\n",
        "  validation_split=0.2,\n",
        "  subset='training',\n",
        "  seed=42,\n",
        "  label_mode='binary',  # There are two classes for this task.\n",
        "  image_size=image_size,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "validation_flowers_dataset = image_dataset_from_directory(\n",
        "  'ABFlowerTrain',\n",
        "  color_mode=colour_mode,\n",
        "  validation_split=0.2,\n",
        "  subset='validation',\n",
        "  seed=42,\n",
        "  label_mode='binary',  # There are two classes for this task.\n",
        "  image_size=image_size,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "test_flowers_dataset = image_dataset_from_directory(\n",
        "  'ABFlowerTest',\n",
        "  shuffle=False,  # Important: the test dataset order must match Kaggle!\n",
        "  labels=None,  # Important: the test dataset has no labels...\n",
        "  color_mode=colour_mode,\n",
        "  image_size=image_size,\n",
        "  batch_size=1)  # Don't batch the test samples.\n",
        "\n",
        "print('Train label names:', train_flowers_dataset.class_names)\n",
        "print('Test label names (empty):', test_flowers_dataset.class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqz78MC9jlBC"
      },
      "source": [
        "Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scqr8DiBjkMU"
      },
      "outputs": [],
      "source": [
        "for images, labels in train_flowers_dataset.take(1):\n",
        "  for i in range(5):\n",
        "    ax = plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype('uint8'))\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR-LuVSw_bh9"
      },
      "source": [
        "Since this is a binary classification task, you can use one-hot or boolean/integer encoding for the targets. \n",
        "\n",
        "The dataset currently uses 0 and 1 for the targets: making the classifier an 'Is artichoke?' (or 'Is buttercup?', depending on ordering) classifier.\n",
        "\n",
        "You can change this to one-hot encoding if you want using a `label_mode` of `categorical`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFWlSGjQ-J3w"
      },
      "source": [
        "Now create a Keras sequential model using the techniques covered in today's lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx5tNhtb-JbS"
      },
      "outputs": [],
      "source": [
        "# To normalise the dataset without first loading the images into memory, we can use a Rescaling layer.\n",
        "# This means images are normalised when given as input to the model.\n",
        "from keras.layers import Rescaling\n",
        "\n",
        "# Your code here: create the model\n",
        "model = Sequential([\n",
        "  Rescaling(1./255),\n",
        "  # Your layers here.\n",
        "])\n",
        "\n",
        "# Your code here: compile the model with suitable losses and metrics.\n",
        "\n",
        "# Now train the model...\n",
        "#model.fit(train_flowers_dataset, validation_data=validation_flowers_dataset, epochs=???)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNdTcRQG-T7X"
      },
      "source": [
        "Now for the predictions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp_uoeQGA_KT"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_flowers_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XgbKHrAqI_h"
      },
      "source": [
        "\n",
        "You may need to threshold or convert from one-hot encoding to get the correct labels, which should be 'A' for artichoke or 'B' for buttercup (not 0 or 1!). See week 6's lab for an example of this.\n",
        "\n",
        "Set `final_predictions` to the final labelled outputs for the test samples, consisting of only 'A's or 'B's.\n",
        "\n",
        "Plot all your test samples and the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP-FCyWco2zx"
      },
      "outputs": [],
      "source": [
        "# Example for an integer output:\n",
        "#final_predictions = ['B' if prediction > 0.5 else 'A' for prediction in predictions]\n",
        "\n",
        "# Example for one-hot:\n",
        "#final_predictions = ['B' if prediction == 1 else 'A' for prediction in np.argmax(predictions, axis=1)]\n",
        "\n",
        "# Visualise the predicted results by ploting all the test images and their predicted labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bdN9IvBh7S"
      },
      "source": [
        "Now you've completed the task, think about the following.\n",
        "\n",
        "For this task, did we really need a CNN? Could we have just averaged the image's colours to identify the flower?\n",
        "\n",
        "If so, when might something like averaging colours not work?\n",
        "\n",
        "\n",
        "\n",
        "### End of the notebook\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
